{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dict_row.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f2434466815d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dict_row.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mdict_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dict_row.json'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "import nltk\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import stem\n",
    "import sklearn.metrics\n",
    "from random import randint\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "with open('../new_userDict.json', 'r') as fp:\n",
    "    \n",
    "    new_userDict = json.load(fp)\n",
    "    \n",
    "    \n",
    "with open('../new_isbnDict.json', 'r') as fp:\n",
    "    \n",
    "    new_isbnDict = json.load(fp)\n",
    "    \n",
    "    \n",
    "with open('../dict_row.json', 'r') as fp:\n",
    "    \n",
    "    dict_row = json.load(fp)\n",
    "    \n",
    "    \n",
    "with open('../dict_col.json', 'r') as fp:\n",
    "    \n",
    "    dict_col = json.load(fp)\n",
    "    \n",
    "print(\"Ok\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ATTENTION!!!  different respect to usersHaveRatedBook(s)\n",
    "\n",
    "def usersHaveRatedBook(new_isbnDict, new_userDict, book_number, score):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    input:  new_isbnDict (Dict), new_userDict (Dict), book_number (int), score (int)\n",
    "    \n",
    "    action: select all users that have given a good rating to book_number\n",
    "    \n",
    "    output: users_rated (list)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    users_rated = []\n",
    "    \n",
    "    for user in new_isbnDict[str(book_number)]:\n",
    "        \n",
    "        if int(new_isbnDict[str(book_number)][user]) > score:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                new_userDict[user]\n",
    "                users_rated.append(user)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                continue\n",
    "            \n",
    "    return list(set(users_rated))\n",
    "\n",
    "\n",
    "def SimilarityUsers(utility_DataFrame, user_number, users_similar):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    input: utility_DataFrame (DataFrame), user_number (int), user_similar (List)\n",
    "    \n",
    "    action: compute cosine similarity between user_number and all the user in users_similar\n",
    "    \n",
    "    output: new_similarity (List of tuples)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    x = utility_DataFrame.loc[str(user_number)]\n",
    "    x_length = norm(x)\n",
    "    \n",
    "    y = utility_DataFrame.loc[users_similar]\n",
    "    y_length = norm(utility_DataFrame.loc[users_similar],axis=1)\n",
    "\n",
    "    \n",
    "    num = (y.values*x.values).sum(axis=1)\n",
    "    den = x_length*y_length\n",
    "\n",
    "    similarity = num/den\n",
    "    similarity = np.nan_to_num(similarity)\n",
    "    \n",
    "    d = list(zip(list(users_similar),similarity))\n",
    "    new_similarity = sorted(d, key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    \n",
    "    return new_similarity\n",
    "\n",
    "def ItemUsersRecommendation(new_similarity, new_userDict, k):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    input:  new_similarity(List of tuples), new_userDict(Dict), score(int), k(int)\n",
    "    \n",
    "    action: recommend items using the ratings of similar users\n",
    "    \n",
    "    output: recommendation (Dict), books (Dict)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if new_similarity == []:\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    if len(new_similarity) > k:\n",
    "    \n",
    "        recommendation = np.mean([u[1] for u in new_similarity[:k]]) \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        recommendation = np.mean([u[1] for u in new_similarity]) \n",
    "        \n",
    "    return recommendation\n",
    "\n",
    "\n",
    "def itemUsersScore(new_similarity,new_isbnDict, book_number, k):\n",
    "    \n",
    "    if new_similarity == []:\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    if len(new_similarity) > k:\n",
    "        \n",
    "        score = [int(new_isbnDict[str(book_number)][u[0]]) for u in new_similarity[:k] if u[1] !=0.0]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        score = [int(new_isbnDict[str(book_number)][u[0]]) for u in new_similarity if u[1] !=0.0]\n",
    "    \n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "def CollaborativeFilteringItemUsersRMSE(utility_DataFrame, new_userDict, new_isbnDict, user_number, score_min, book_number, k):\n",
    "    \n",
    "    users_rated_book = usersHaveRatedBook(new_isbnDict, new_userDict, book_number, score_min)\n",
    "    \n",
    "    if users_rated_book == []:\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    new_similarity = SimilarityUsers(utility_DataFrame, user_number, users_rated_book)\n",
    "    \n",
    "    \n",
    "    if new_similarity == []:\n",
    "\n",
    "        return None\n",
    "    \n",
    "    score = itemUsersScore(new_similarity,new_isbnDict, book_number, k)\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSampleDict(new_isbnDict, new_userDict, t1, t2):\n",
    "    \n",
    "\n",
    "    small_isbnDict = {}\n",
    "\n",
    "    for book in new_isbnDict:\n",
    "\n",
    "        temp = new_isbnDict[book]\n",
    "        i = 0\n",
    "\n",
    "        for t in temp.values():\n",
    "            if t != \"0\":\n",
    "                i = i+1\n",
    "\n",
    "        if i >t1:\n",
    "\n",
    "            small_isbnDict[book] = new_isbnDict[book]   \n",
    "\n",
    "    small_userDict = {}\n",
    "\n",
    "    for user in new_userDict:\n",
    "\n",
    "        temp = new_userDict[user]\n",
    "        i = 0\n",
    "\n",
    "        for t in temp.values():\n",
    "            if t != \"0\":\n",
    "                i = i+1\n",
    "\n",
    "        if i >t2:\n",
    "\n",
    "            small_userDict[user] = new_userDict[user]\n",
    "            \n",
    "            \n",
    "    return small_isbnDict, small_userDict\n",
    "\n",
    "\n",
    "def computeMatrices(train_userDict,train_isbnDict,small_userDict,small_isbnDict, dict_row, dict_col):\n",
    "\n",
    "    n = len(small_isbnDict)\n",
    "    m = len(small_userDict)\n",
    "    \n",
    "    index = sorted(small_userDict.keys())\n",
    "    columns = sorted(small_isbnDict.keys())\n",
    "\n",
    "    dict_row = {k:v for v,k in enumerate(index)}\n",
    "    dict_col = {k:v for v,k in enumerate(columns)}\n",
    "\n",
    "    u = np.zeros((m,n)) \n",
    "    R = np.zeros((m,n))\n",
    "    for user in train_userDict:\n",
    "        for isbn in train_userDict[user]:\n",
    "            try:\n",
    "                u[dict_row[user]][dict_col[isbn]] = train_userDict[user][isbn]\n",
    "                R[dict_row[user]][dict_col[isbn]] = 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    for isbn in train_isbnDict:\n",
    "        for user in train_isbnDict[isbn]:\n",
    "            try:\n",
    "                u[dict_row[user]][dict_col[isbn]] = train_isbnDict[isbn][user]\n",
    "                R[dict_row[user]][dict_col[isbn]] = 1\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    small_utility_DataFrame = pd.DataFrame(u, index = index, columns = columns)\n",
    "    R = pd.DataFrame(R, index = index, columns = columns)\n",
    "\n",
    "    return u, R, small_utility_DataFrame\n",
    "\n",
    "\n",
    "def selectSample(i,b,list_user,small_userDict, small_isbnDict):\n",
    "\n",
    "    test_index = [k for k in range((i*b),(i+1)*b)]\n",
    "    train_index = [k for k in range(len(list_user)) if k not in test_index]\n",
    "\n",
    "    train = list_user[train_index]\n",
    "    test  = list_user[test_index]\n",
    "\n",
    "    train_userDict = {}\n",
    "    train_isbnDict = {isbn:{} for isbn in small_isbnDict}\n",
    "\n",
    "    for user in train:\n",
    "\n",
    "        train_userDict[user] = small_userDict[user]\n",
    "\n",
    "    for isbn in small_isbnDict:\n",
    "        for user in small_isbnDict[isbn]:\n",
    "\n",
    "            try:\n",
    "                train_userDict[user]\n",
    "                train_isbnDict[isbn][user] = small_isbnDict[isbn][user]\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "                \n",
    "    test_userDict = {}\n",
    "    test_isbnDict = {isbn:{} for isbn in small_isbnDict}\n",
    "\n",
    "    for user in test:\n",
    "\n",
    "        test_userDict[user] = small_userDict[user]\n",
    "\n",
    "    for isbn in small_isbnDict:\n",
    "        for user in small_isbnDict[isbn]:\n",
    "\n",
    "            try:\n",
    "                test_userDict[user]\n",
    "                test_isbnDict[isbn][user] = small_isbnDict[isbn][user]\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    return train_userDict, train_isbnDict, test_userDict, test_isbnDict, test\n",
    "\n",
    "\n",
    "#indices = [i for i in range(len(list_user))]\n",
    "#indices = np.array(indices)\n",
    "#k = 5\n",
    "#b = len(small_userDict)//k\n",
    "\n",
    "\n",
    "def selectSampleRandom(indices,b,list_user,small_userDict, small_isbnDict):\n",
    "    \n",
    "    test_index = np.random.choice(indices, size = b, replace=False)\n",
    "    \n",
    "    indices = np.delete(indices,test_index)\n",
    "    \n",
    "    train_index = [k for k in range(len(list_user)) if k not in test_index]\n",
    "    \n",
    "\n",
    "    train = list_user[train_index]\n",
    "    test  = list_user[test_index]\n",
    "\n",
    "    train_userDict = {}\n",
    "    train_isbnDict = {isbn:{} for isbn in small_isbnDict}\n",
    "\n",
    "    for user in train:\n",
    "\n",
    "        train_userDict[user] = small_userDict[user]\n",
    "\n",
    "    for isbn in small_isbnDict:\n",
    "        for user in small_isbnDict[isbn]:\n",
    "\n",
    "            try:\n",
    "                train_userDict[user]\n",
    "                train_isbnDict[isbn][user] = small_isbnDict[isbn][user]\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    return train_userDict, train_isbnDict, test_userDict, test_isbnDict, test, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mainItemUsersRMSE(utility_DataFrame, R, new_userDict, new_isbnDict, score_min=0, k=3):\n",
    "    \n",
    "\n",
    "    rmse_dict = {j:{} for j in new_userDict.keys()}\n",
    "\n",
    "    for user in new_userDict:\n",
    "\n",
    "        for book in new_userDict[user]:\n",
    "\n",
    "            if new_userDict[user][book] != \"0\":\n",
    "\n",
    "                try:\n",
    "                    new_isbnDict[book]\n",
    "                    rmse_dict[user][book] = new_userDict[user][book]\n",
    "                except:\n",
    "\n",
    "                    continue\n",
    "\n",
    "    d = rmse_dict.keys()\n",
    "    v = []\n",
    "\n",
    "    for key in d:\n",
    "\n",
    "        if rmse_dict[key] == {}:\n",
    "\n",
    "            v.append(key)\n",
    "\n",
    "    for i in v:\n",
    "\n",
    "        del rmse_dict[i]\n",
    "\n",
    "    rmse_vector_itemUsers = []\n",
    "    \n",
    "    #i = 0\n",
    "    \n",
    "    for user in rmse_dict:\n",
    "        \n",
    "        #i = i+1\n",
    "        #if i%1000 == 0:\n",
    "            \n",
    "        #    print(\"Pause\")\n",
    "        #    time.sleep(30)\n",
    "\n",
    "        for isbn in rmse_dict[user]:\n",
    "            \n",
    "            prediction_score = None\n",
    "            user_number = user\n",
    "            book_number = isbn\n",
    "            \n",
    "            try:\n",
    "                prediction_score = CollaborativeFilteringItemUsersRMSE(utility_DataFrame, new_userDict, new_isbnDict, \n",
    "                                                                   user_number, score_min, book_number, k)\n",
    "                \n",
    "                #print(prediction_score)\n",
    "                true_score = utility_DataFrame[book_number][user_number]\n",
    "                \n",
    "                if prediction_score == None:\n",
    "                    \n",
    "                    term1 = utility_DataFrame[book_number]\n",
    "                    r1 = R[book_number]\n",
    "                    term1 = term1[r1 ==1]\n",
    "                    \n",
    "                    term2 = utility_DataFrame.loc[user_number]\n",
    "                    r2 = R.loc[user_number]\n",
    "                    term2 = term2[r2==1]\n",
    "                    \n",
    "                    prediction_score = (np.mean(term1) + np.mean(term2))/2\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if prediction_score != None:\n",
    "\n",
    "                rmse_vector_itemUsers.append(tuple([user_number, book_number, prediction_score, true_score]))\n",
    "\n",
    "\n",
    "    rmse = np.sqrt((np.array([u[2]-u[3] for u in rmse_vector_itemUsers])**2).sum()/len(rmse_vector_itemUsers))\n",
    "    \n",
    "    return rmse, rmse_vector_itemUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u, R, small_utility_DataFrame = computeMatrices(small_userDict,small_isbnDict,small_userDict,small_isbnDict,dict_row,dict_col)\n",
    "\n",
    "rmse_test, rmse_vector_itemUsers = mainItemUsersRMSE(small_utility_DataFrame,R,small_userDict,small_isbnDict,score_min=0,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_vector_itemUsers[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K-fold cross validation for collaborative filtering user-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def itemUsers_KfoldsCV(small_userDict,small_isbnDict, dict_row, dict_col, kk):\n",
    "\n",
    "    RMSE = []\n",
    "    #kk = 5\n",
    "    list_user = np.array(sorted(small_userDict.keys()))\n",
    "    b = len(small_userDict)//kk\n",
    "\n",
    "    for iterator in range(kk):\n",
    "\n",
    "        train_userDict, train_isbnDict, test_userDict, test_isbnDict, test =  selectSample(iterator,b,list_user,\n",
    "                                                                                           small_userDict, small_isbnDict)\n",
    "\n",
    "        u, R, small_utility_DataFrame = computeMatrices(train_userDict,small_isbnDict,small_userDict,small_isbnDict,dict_row,dict_col)\n",
    "\n",
    "        rmse_test, rmse_vector_itemUsers = mainItemUsersRMSE(small_utility_DataFrame,R,test_userDict,test_isbnDict,score_min=0,k=3)\n",
    "        print(\"Ok\")\n",
    "\n",
    "        RMSE.append(rmse_test)\n",
    "        \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_userDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b1047adb8dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmall_userDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_userDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_userDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msmall_isbnDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_isbnDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_isbnDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_userDict' is not defined"
     ]
    }
   ],
   "source": [
    "small_userDict = {k:new_userDict[k] for k in list(new_userDict.keys())[:5000]}\n",
    "small_isbnDict = {k:new_isbnDict[k] for k in list(new_isbnDict.keys())[:5000] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_isbnDict, small_userDict = createSampleDict(new_isbnDict, new_userDict, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RMSE = itemUsers_KfoldsCV(new_userDict,new_isbnDict, dict_row, dict_col, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"rmse collaborative filtering user-based: \",np.mean(RMSE))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
